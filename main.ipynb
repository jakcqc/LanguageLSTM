{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this notebook where we'll be implementing a simple RNN character model with PyTorch to familiarize ourselves with the PyTorch library and get started with RNNs. You can run the code weâ€™re using on FloydHub by clicking the button below and creating the project as well.\n",
    "\n",
    "[![Run on FloydHub](https://static.floydhub.com/button/button-small.svg)](https://floydhub.com/run?template=https://github.com/gabrielloye/RNN-walkthrough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, we'll be building a model that can complete your sentence based on a few characters or a word used as input.\n",
    "![Example](img/Slide4.jpg)\n",
    "To keep this short and simple, we won't be using any large or external datasets. Instead, we'll just be defining a few sentences to see how the model learns from these sentences. The process that this implementation will take is as follows:\n",
    "![Overview](img/Slide5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off by importing the main PyTorch package along with the *Variable* class used to store our data tensors and the *nn* package which we will use when building the model. In addition, we'll only be using numpy to pre-process our data as Torch works really well with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll define the sentences that we want our model to output when fed with the first word or the first few characters.\n",
    "\n",
    "Then we'll create a dictionary out of all the characters that we have in the sentences and map them to an integer. This will allow us to convert our input characters to their respective integers (*char2int*) and vice versa (*int2char*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['terrain ujwo with position 74980 and amount 1173 to 9892 and class(s) owjk and position in jjlih and material(s) dicgxwdt and rotation 469-20-892 and amount 3885 to 7331 ', 'skylight wvdcby with intensity 542 lux as well as object lklez with rotation 3-4-4 to 860-0-7  ', 'object ymus with visibility false  as well as skylight pcjg with rotation 02-08-8 ', 'skylight sezkcghs with intensity 800 lux as well as terrain gkur with rotation 36-93-875 to 139-059-970 and material(s) rnvxtwif-fepf and material(s) aubkfiuti-ghbs ', 'skylight wevr with intensity 113 lux ', 'object rsag with rotation 302-9-02 to 785-9-896  ', 'terrain hvdjv with rotation 1-5-975 to 426-792-67 ', 'object eeeogbzpz with rotation 04-6-670 and overlap false and extent 1-969-15 and amount 7994 to 3020 and position 1048715 and scale 8 and visibility false and extent 928-588-5 and material(s) yllhovito-lfrwi and rotation 532-6-4 to 75-1-78 and class(s) ompqclqf-slaep and amount 7353 to 4489 and visibility false and class(s) yyryu-xcllczmmjm-vqzmyyn and amount 5058 to 0459 and material(s) nzkd-qeiytbdqducascz-htqpmm  as well as terrain hrxy with position 68120 and overlap false and rotation 3-66-5 to 761-041-17 ', 'directionallight kditf with rotation 18-0-80 as well as object nrtx with scale 4 and visibility true  ', 'directionallight eeqxz with rotation 7-55-3 and intensity 057 lux ', 'directionallight tybl with rotation 918-32-280 to 1-557-58 ', 'object hoxa with scale 9  ', 'terrain xhxp with scale 4 to 3  ', 'directionallight rbbosxy with intensity 184 lux ', 'object igkx with class(s) wvysn-sseqry  ', 'directionallight tzro with intensity 295 lux ', 'object lmlj with overlap true  as well as skylight lmin with intensity 451 lux and rotation 062-75-59 to 289-0-00 ', 'directionallight fefh with rotation 0-3-642 and intensity 097 lux as well as terrain wbxavkzzsp with rotation 8-71-4 to 87-73-94 ', 'terrain ayon with scale 6 to 8  and visibility false and class(s) zuyw-agryzmrmijn-ynnyi-byldb as well as directionallight jpar with intensity 690 lux and intensity 853 lux and rotation 7-059-3 to 7-71-7 and intensity 659 lux and rotation 26-538-8 to 513-493-8 ', 'terrain ojsm with extent 805-51-13 ', 'directionallight stha with rotation 380-8-042 as well as skylight dgok with intensity 529 lux and rotation 87-92-321 ', 'object luhcnuixffgn with material(s) rkzin-wlerdd-kyqp  ', 'skylight fkuwm with rotation 96-152-24 to 258-0-47 as well as object vwwzk with position 7118  ', 'object ddtw with material(s) imnhtpwfb-kzpg-tapuw and material(s) bhtbl-ncuk  ', 'object utnw with visibility true and material(s) cxhxzlfgqsbqy-qkfy and rotation 9-8-001  ', 'directionallight dvig with rotation 3-895-710 as well as directionallight nfyot with intensity 203 lux ', 'directionallight oimm with rotation 02-179-2 ', 'skylight qcizzx with rotation 9-20-975 to 9-627-51 and rotation 885-046-95 to 26-49-9 and intensity 224 lux as well as terrain hockr with extent 9-115-362 ', 'terrain dscs with rotation 05-32-84 as well as terrain sxtsc with position 4772002 and class(s) yrcmskps-cyuvr and class(s) kyvtxxd and extent 822-3-62 and amount 9914 to 9317 and extent 16-3-97 and scale 0 and overlap false and material(s) sjie and position above yxsip-above qibsh-on ebkr-on bxau ', 'directionallight bucbjro with rotation 001-32-90 to 491-402-98 ', 'terrain hgtgaqo with material(s) warcafhizw-ocgzjvgav ', 'terrain ngntdg with scale 9 to 4  as well as directionallight zzwzf with rotation 17-69-68 to 01-480-8 and rotation 55-799-96 ', 'skylight rviti with rotation 4-54-9 ', 'skylight vwndwoipj with intensity 819 lux as well as terrain idzbl with scale 0 to 7  and visibility true and visibility false ', 'directionallight wtwa with intensity 160 lux as well as directionallight fgpz with rotation 38-1-35 ', 'directionallight ygoqjbdy with intensity 344 lux as well as directionallight ftnw with intensity 433 lux ', 'object hatdyz with scale 5 and scale 0 to 8  and class(s) botjg-wonuq and rotation 72-472-184 and scale 2 and visibility false and scale 1 to 9  and scale 6 to 1  and visibility true and visibility false and visibility true and position 414583 and position 3169 and material(s) xikoon-jrzn-qqvrlt and material(s) tadqrh-bnieg and extent 5-1-4  ', 'directionallight sgko with intensity 174 lux ', 'directionallight efau with rotation 925-99-59 as well as terrain somr with amount 2102 to 4693 and class(s) xxnsenznqpmr-lavxq ', 'directionallight dxptq with rotation 4-3-45 to 972-69-2 ', 'object ulqyh with class(s) lpqgonmz-tqefew and rotation 1-924-1 and scale 8 to 1  and overlap true and rotation 4-636-850 and overlap false and overlap false and class(s) wbws-akhnj  as well as object xnzo with scale 3 to 2  and visibility true and class(s) zenp-eilbf and class(s) gedmmrx  ', 'object nwul with amount 0691 to 7462 and overlap true  as well as terrain puxhx with rotation 7-648-680 ', 'skylight udrm with intensity 145 lux as well as object qawk with rotation 898-94-510 to 69-180-8  ', 'directionallight tzht with rotation 6-750-4 to 6-667-3 and intensity 288 lux and rotation 939-6-838 to 640-262-747 as well as object vfshizr with rotation 2-3-14 and class(s) mtekkelvdwh-obruu  ', 'directionallight qrwh with intensity 224 lux and intensity 927 lux as well as directionallight zybjb with rotation 4-6-2 ', 'object rmuj with overlap false and overlap true and scale 3 and amount 5866 to 3700 and material(s) spbwyodwh-jmnp and overlap false and visibility false  as well as directionallight zefo with rotation 40-74-50 to 016-9-455 ', 'directionallight eccrzsqft with intensity 040 lux ', 'terrain hjkw with amount 4816 to 6007 ', 'terrain nxppa with material(s) yggo-nibeqzrav-qbyz ', 'skylight couca with rotation 39-7-2 to 435-838-27 ', 'directionallight zqkaz with rotation 62-6-84 to 839-8-449 and intensity 748 lux and intensity 357 lux and intensity 808 lux as well as terrain czol with position 883775 and position 426160 and amount 4380 to 4756 ', 'object dcgf with amount 3536 to 7611  as well as terrain nsfy with extent 646-906-2 ', 'object jyuuus with amount 7349 to 4899 and amount 6706 to 5936  ', 'object xzvwf with rotation 87-037-73 and extent 4-22-8 and overlap true and extent 2-70-740 and visibility false and amount 6632 to 1961 and class(s) aiiwnt-lcom and scale 6 and position 9728273 and amount 2740 to 1597 and overlap true and visibility false and class(s) zeotqshgersvcj-gcwlrvirrkgn-rltkd and material(s) vuil and material(s) wfajmou-yyjkcozfa-bcayj and overlap false and extent 03-901-9 and rotation 54-648-361 to 99-776-549 and position 7656 and position 9484190 and amount 1208 to 0621 and extent 22-429-188  as well as terrain xgdctul with visibility false ', 'object cotq with scale 6 to 7   ', 'directionallight xelwfm with intensity 099 lux as well as terrain ifxjcr with amount 2307 to 0934 and rotation 45-5-79 and class(s) czzojwjoar-jmqnecugv-kool ', 'skylight qvasusiy with rotation 822-571-720 to 2-36-1 and rotation 124-4-13 to 96-21-74 as well as terrain jzer with extent 95-65-8 ', 'terrain vgya with class(s) rpjbjhap-gpme as well as directionallight tkhc with intensity 864 lux ', 'directionallight sztv with intensity 237 lux and rotation 137-545-09 to 3-729-947 ', 'skylight baddh with intensity 986 lux and intensity 805 lux as well as skylight qnqs with intensity 719 lux ', 'directionallight emzy with rotation 1-6-87 to 23-407-28 and intensity 775 lux and intensity 018 lux ', 'directionallight djck with rotation 5-1-49 ', 'skylight dtuce with intensity 658 lux ', 'directionallight chko with rotation 010-9-0 and rotation 2-9-848 and intensity 085 lux and rotation 16-09-35 to 947-5-36 and intensity 945 lux and intensity 795 lux and intensity 398 lux and intensity 538 lux ', 'object jdztz with rotation 645-673-10 to 008-533-940 and scale 3 to 6  and material(s) fuvsufd-vyjho and rotation 413-08-37  as well as terrain ifit with amount 7919 to 8276 ', 'terrain ihry with extent 46-211-5 as well as terrain dxaeo with scale 3 and class(s) cbly-nkpq-tybawtzng-jmfo-ygyn ', 'directionallight cknq with rotation 1-7-9 and rotation 94-88-03 to 000-837-3 and rotation 42-55-18 to 2-27-636 and intensity 686 lux and rotation 22-84-18 and intensity 895 lux as well as skylight eevc with rotation 994-00-39 ', 'directionallight wahu with rotation 9-0-795 as well as terrain vves with overlap false ', 'terrain nhktl with material(s) bnukvqs as well as object tfjzr with extent 2-27-084  ', 'skylight jhjvc with rotation 1-524-2 to 04-205-435 ', 'object skfctazq with class(s) guki-ejwdomnx  ', 'directionallight plcnwps with intensity 623 lux and rotation 4-65-9 ', 'object nmfxxtu with class(s) wjroezibn-lcuopt  as well as terrain kxiz with overlap false ', 'object cmqu with material(s) oypmcrhlv-zoua and visibility true  as well as object rgey with scale 4 to 5   ', 'skylight zrwhuyi with rotation 99-69-3 to 675-1-3 as well as terrain ingb with rotation 21-47-0 to 9-877-579 and scale 7 and position 842941 and material(s) woqiswzrourz-uigo and amount 8617 to 7417 and extent 191-376-0 ', 'terrain hnjvr with extent 621-522-314 and rotation 4-0-8 to 015-465-562 ', 'terrain krog with overlap false ', 'object caky with extent 48-762-40  as well as terrain bjjz with position on ykcqm-on kgcil-above bdcel-on jzhr-in fefc-in gqdn and extent 2-695-20 ', 'skylight wioer with intensity 863 lux ', 'skylight peix with rotation 633-7-69 to 13-080-091 and rotation 854-00-4 as well as object kxqd with rotation 0-0-5 to 5-05-01  ', 'terrain jbenr with overlap true ', 'skylight xjunz with rotation 8-6-96 to 133-96-6 as well as object thxm with material(s) nexkus-imil and rotation 3-56-425  ', 'directionallight nrohvia with intensity 284 lux as well as terrain jmgb with rotation 584-341-2 ', 'object veho with rotation 519-99-765  as well as directionallight zave with intensity 368 lux ', 'terrain xxube with visibility true as well as skylight dgjf with intensity 133 lux and intensity 994 lux ', 'object nndf with rotation 281-75-355  ', 'directionallight myum with rotation 45-672-693 as well as skylight tezg with rotation 37-34-751 ', 'terrain mlnnbw with rotation 71-775-405 ', 'directionallight ynzpoj with rotation 35-985-55 to 5-8-7 as well as directionallight wtiq with intensity 571 lux ', 'terrain baglpe with scale 8 and rotation 96-4-1 and class(s) tzrr and scale 0 and scale 3 to 9  and visibility true and overlap false and overlap true and extent 233-150-3 and material(s) wlyrivc and rotation 29-36-061 and position 031106 and visibility false and rotation 198-534-9 to 84-115-874 as well as object rmoy with amount 8718 to 2088 and amount 5565 to 6426 and amount 1518 to 3041 and class(s) nrpsl and extent 02-251-3 and scale 8 to 1  and scale 8 to 2  and visibility false and extent 87-7-592 and position in iwqa-above rmnbvqlerc-on ntxgzjzzc-above nwbldtb-in hvqeymup-on ottknb-in xdnivpy-in uoingu-on vdiw-on cgvnp-above okkk-in eost-above jvtl-in njan-above cbpic-above kfzcw-on cnga-above vxto-above zhldcg-on cmcj and material(s) jtdxwt-vxkt and scale 5 to 4  and rotation 5-888-666 and scale 6 to 1  and material(s) wbryybokydq-jawb and extent 44-6-7 and visibility true and class(s) ihfaalnfaqg-ktfq-flrwq-yvkl and amount 0387 to 0493 and material(s) yclw and visibility true and overlap false and extent 6-57-73 and scale 0 to 8  and scale 0 and material(s) mlrjvj-wrixq and extent 59-9-2 and material(s) krqssyigsthnj-mxan  ', 'object ntgjuo with amount 3746 to 3687  ', 'directionallight ormg with rotation 37-918-085 to 257-4-95 and intensity 762 lux and intensity 179 lux as well as directionallight fkhm with intensity 411 lux ', 'terrain wjazhybh with class(s) sqsxbxnvec-ljbtkmwspn-iudus and position in dbfo-in rzkp-in zemjsv-above cvlorg-on zdxc-on cysj-in jlfumb-above rhhe-in jmhn-on xucz-on qxvf-above btvl-on utlessve-in twypfdtwg-on oibb ', 'directionallight sintpgue with rotation 7-8-5 as well as directionallight zwxweh with intensity 187 lux and intensity 721 lux and intensity 445 lux and intensity 244 lux and intensity 851 lux and intensity 847 lux and intensity 778 lux ', 'object enfhmpj with material(s) ngkbvuemcle-pfveqgzo and amount 8098 to 4300 and class(s) wbfhbjoml-spzr-sttyq-ivit and amount 6365 to 0971 and rotation 789-8-73 to 663-036-0 and rotation 426-449-65 and position above ukyu-in broc-on pykb-on mifx-above iwxz-in oifw-above yfgdb-above icvh-on ckbjmq-in kzod-on ctfm-above hiopc-in dfnqt-in issne-in ltxz-on kbnzg-above wkyy-in rxuve-above pxact-on wsvfod-on ptgmu-on ayml-on ujvr-above tbnku-above ztku-above lndn-in vrgi-on hbotz-in iwdlt-on ojzx-above ulke-in ehuo-in exilwd-above mouojf-in slxbxwmf-in byfr-above zibc-on fcfa-on hdgo-above peuwz-on wsgn-on gekfzc-above snmh-in pzpa-above jbnik-in rexfzl-above ayvb-on gcwoj-in skxa-on mnbn-on kgci-in mepak-on nseo-above zaysopo-on pldv-in tzkt-above mqsrf-on bmox-above fhvsa-on jzdsi-in tznq-above dmclj-in bfqm-in qvvxg-above gall-above lyxw  ', 'object wvrxtvhgt with overlap true  ', 'terrain amvy with material(s) dusux-fmtz and position above wynue and rotation 561-2-3 ', 'terrain gusiis with scale 2 to 3  and material(s) lgyj-ynvln and extent 063-71-01 and overlap true ', 'directionallight syav with intensity 240 lux as well as skylight iios with intensity 621 lux ', 'terrain lczagyju with amount 7169 to 7910 and scale 6 to 6  as well as object fjvv with visibility true  ', 'terrain:{\\\\nujwo:{\\\\nposition:{ 74980 }--\\\\namount:[ 1173]-- [9892] --/nclass(s):[ owjk ]--\\\\n position:{ \\\\nin: jjlih }--\\\\nmaterial(s):[ dicgxwdt] --\\\\nrotation:[469-20-892] --\\\\n amount:[ 3885]-- [7331] }', 'skylight:{\\\\nwvdcby:{\\\\nintensity: 542 }--\\\\nobject:{ \\\\nlklez:{\\\\nrotation:[3-4-4 ] -- [860-0-7]  ', 'object:{ \\\\nymus:{\\\\nvisibility: false   }--\\\\nskylight:{\\\\npcjg:{\\\\nrotation: 02-08-8', 'skylight:{\\\\nsezkcghs:{\\\\nintensity: 800 }--\\\\nterrain:{\\\\ngkur:{\\\\nrotation:[36-93-875 ] -- [139-059-970] --\\\\nmaterial(s):[ rnvxtwif-fepf] --\\\\nmaterial(s):[ aubkfiuti-ghbs] ', 'skylight:{\\\\nwevr:{\\\\nintensity: 113}', 'object:{ \\\\nrsag:{\\\\nrotation:[302-9-02 ] -- [785-9-896]  }', 'terrain:{\\\\nhvdjv:{\\\\nrotation:[1-5-975 ] -- [426-792-67] }', 'object:{ \\\\neeeogbzpz:{\\\\nrotation:[04-6-670] --\\\\noverlap: false --/nextent:[ 1-969-15] --\\\\namount:[ 7994]-- [3020] --\\\\nposition:{ 1048715 }--\\\\nscale: [8] --/nvisibility: false --\\\\n extent:[ 928-588-5] --\\\\nmaterial(s):[ yllhovito-lfrwi] --\\\\nrotation:[532-6-4 ] -- [75-1-78] --\\\\nclass(s):[ ompqclqf-slaep ]--\\\\namount:[ 7353]-- [4489] --\\\\nvisibility: false --\\\\nclass(s):[ yyryu-xcllczmmjm-vqzmyyn ]--\\\\namount:[ 5058]-- [0459] --\\\\n material(s):[ nzkd-qeiytbdqducascz-htqpmm]   }--\\\\nterrain:{\\\\nhrxy:{\\\\nposition:{ 68120 }--\\\\noverlap: false --\\\\n rotation:[3-66-5 ] -- [761-041-17] ', 'directional_light:{\\\\nkditf:{\\\\nrotation: 18-0-80 }--\\\\nobject:{ \\\\nnrtx:{\\\\nscale: [4] --\\\\nvisibility: true  ', 'directional_light:{\\\\neeqxz:{\\\\nrotation: 7-55-3--\\\\nintensity: 057}}', 'directional_light:{\\\\ntybl:{\\\\nrotation: 918-32-280 ] -- [1-557-58}', 'object:{ \\\\nhoxa:{\\\\nscale: [9]  }', 'terrain:{\\\\nxhxp:{\\\\nscale: [4 -- 3 ] }', 'directional_light:{\\\\nrbbosxy:{\\\\nintensity: 184}', 'object:{ \\\\nigkx:{\\\\nclass(s):[ wvysn-sseqry ] }', 'directional_light:{\\\\ntzro:{\\\\nintensity: 295}', 'object:{ \\\\nlmlj:{\\\\noverlap: true   }--\\\\nskylight:{\\\\nlmin:{\\\\nintensity: 451--\\\\nrotation: 062-75-59 ] -- [289-0-00}', 'directional_light:{\\\\nfefh:{\\\\nrotation: 0-3-642--\\\\nintensity: 097} }--\\\\nterrain:{\\\\nwbxavkzzsp:{\\\\nrotation:[8-71-4 ] -- [87-73-94] ', 'terrain:{\\\\nayon:{\\\\nscale: [6 -- 8 ] --\\\\nvisibility: false --\\\\nclass(s):[ zuyw-agryzmrmijn-ynnyi-byldb ] }--\\\\ndirectional_light:{\\\\njpar:{\\\\nintensity: 690--\\\\nintensity: 853--\\\\nrotation: 7-059-3 ] -- [7-71-7--\\\\nintensity: 659}}}--\\\\nrotation: 26-538-8 ] -- [513-493-8}', 'terrain:{\\\\nojsm:{\\\\nextent:[ 805-51-13] }', 'directional_light:{\\\\nstha:{\\\\nrotation: 380-8-042 }--\\\\nskylight:{\\\\ndgok:{\\\\nintensity: 529--\\\\nrotation: 87-92-321}', 'object:{ \\\\nluhcnuixffgn:{\\\\nmaterial(s):[ rkzin-wlerdd-kyqp]  }', 'skylight:{\\\\nfkuwm:{\\\\nrotation: 96-152-24 ] -- [258-0-47 }--\\\\nobject:{ \\\\nvwwzk:{\\\\nposition:{ 7118 } ', 'object:{ \\\\nddtw:{\\\\nmaterial(s):[ imnhtpwfb-kzpg-tapuw] --\\\\nmaterial(s):[ bhtbl-ncuk]  }', 'object:{ \\\\nutnw:{\\\\nvisibility: true --\\\\nmaterial(s):[ cxhxzlfgqsbqy-qkfy] --\\\\nrotation:[9-8-001]  }', 'directional_light:{\\\\ndvig:{\\\\nrotation: 3-895-710 }--\\\\ndirectional_light:{\\\\nnfyot:{\\\\nintensity: 203', 'directional_light:{\\\\noimm:{\\\\nrotation: 02-179-2}', 'skylight:{\\\\nqcizzx:{\\\\nrotation: 9-20-975 ] -- [9-627-51--\\\\nrotation: 885-046-95 ] -- [26-49-9--\\\\nintensity: 224}} }--\\\\nterrain:{\\\\nhockr:{\\\\nextent:[ 9-115-362] ', 'terrain:{\\\\ndscs:{\\\\nrotation:[05-32-84]  }--\\\\nterrain:{\\\\nsxtsc:{\\\\nposition:{ 4772002 }--\\\\nclass(s):[ yrcmskps-cyuvr ]--\\\\n class(s):[ kyvtxxd ]--\\\\n extent:[ 822-3-62] --\\\\namount:[ 9914]-- [9317] --\\\\nextent:[ 16-3-97] --\\\\nscale: [0] --\\\\noverlap: false --\\\\nmaterial(s):[ sjie] --\\\\n position:{ \\\\nabove: yxsip--\\\\nabove: qibsh--\\\\non: ebkr--\\\\non: bxau }', 'directional_light:{\\\\nbucbjro:{\\\\nrotation: 001-32-90 ] -- [491-402-98}', 'terrain:{\\\\nhgtgaqo:{\\\\nmaterial(s):[ warcafhizw-ocgzjvgav] }', 'terrain:{\\\\nngntdg:{\\\\nscale: [9 -- 4 ]  }--\\\\ndirectional_light:{\\\\nzzwzf:{\\\\nrotation: 17-69-68 ] -- [01-480-8--\\\\nrotation: 55-799-96}', 'skylight:{\\\\nrviti:{\\\\nrotation: 4-54-9}', 'skylight:{\\\\nvwndwoipj:{\\\\nintensity: 819 }--\\\\nterrain:{\\\\nidzbl:{\\\\nscale: [0 -- 7 ] --\\\\nvisibility: true --\\\\nvisibility: false ', 'directional_light:{\\\\nwtwa:{\\\\nintensity: 160 }--\\\\ndirectional_light:{\\\\nfgpz:{\\\\nrotation: 38-1-35', 'directional_light:{\\\\nygoqjbdy:{\\\\nintensity: 344 }--\\\\ndirectional_light:{\\\\nftnw:{\\\\nintensity: 433', 'object:{ \\\\nhatdyz:{\\\\nscale: [5] --\\\\nscale: [0 -- 8 ] --\\\\nclass(s):[ botjg-wonuq ]--/nrotation:[72-472-184] --\\\\nscale: [2] --\\\\nvisibility: false --\\\\nscale: [1 -- 9 ] --\\\\nscale: [6 -- 1 ] --\\\\nvisibility: true --\\\\nvisibility: false --\\\\nvisibility: true --/nposition:{ 414583 }--/nposition:{ 3169 }--\\\\n material(s):[ xikoon-jrzn-qqvrlt] --\\\\nmaterial(s):[ tadqrh-bnieg] --\\\\nextent:[ 5-1-4]  }', 'directional_light:{\\\\nsgko:{\\\\nintensity: 174}', 'directional_light:{\\\\nefau:{\\\\nrotation: 925-99-59 }--\\\\nterrain:{\\\\nsomr:{\\\\namount:[ 2102]-- [4693] --\\\\n class(s):[ xxnsenznqpmr-lavxq ]', 'directional_light:{\\\\ndxptq:{\\\\nrotation: 4-3-45 ] -- [972-69-2}', 'object:{ \\\\nulqyh:{\\\\nclass(s):[ lpqgonmz-tqefew ]--\\\\nrotation:[1-924-1] --/nscale: [8 -- 1 ] --\\\\n overlap: true --\\\\nrotation:[4-636-850] --\\\\noverlap: false --\\\\noverlap: false --\\\\n class(s):[ wbws-akhnj ]  }--\\\\nobject:{ \\\\nxnzo:{\\\\nscale: [3 -- 2 ] --\\\\nvisibility: true --\\\\n class(s):[ zenp-eilbf ]--/nclass(s):[ gedmmrx ] ', 'object:{ \\\\nnwul:{\\\\namount:[ 0691]-- [7462] --\\\\noverlap: true   }--\\\\nterrain:{\\\\npuxhx:{\\\\nrotation:[7-648-680] ', 'skylight:{\\\\nudrm:{\\\\nintensity: 145 }--\\\\nobject:{ \\\\nqawk:{\\\\nrotation:[898-94-510 ] -- [69-180-8]  ', 'directional_light:{\\\\ntzht:{\\\\nrotation: 6-750-4 ] -- [6-667-3--\\\\nintensity: 288}--\\\\nrotation: 939-6-838 ] -- [640-262-747} }--\\\\nobject:{ \\\\nvfshizr:{\\\\nrotation:[2-3-14] --/nclass(s):[ mtekkelvdwh-obruu ] ', 'directional_light:{\\\\nqrwh:{\\\\nintensity: 224--\\\\nintensity: 927} }--\\\\ndirectional_light:{\\\\nzybjb:{\\\\nrotation: 4-6-2', 'object:{ \\\\nrmuj:{\\\\noverlap: false --\\\\noverlap: true --\\\\nscale: [3] --\\\\n amount:[ 5866]-- [3700] --\\\\nmaterial(s):[ spbwyodwh-jmnp] --\\\\noverlap: false --\\\\n visibility: false   }--\\\\ndirectional_light:{\\\\nzefo:{\\\\nrotation: 40-74-50 ] -- [016-9-455', 'directional_light:{\\\\neccrzsqft:{\\\\nintensity: 040}', 'terrain:{\\\\nhjkw:{\\\\namount:[ 4816]-- [6007] }', 'terrain:{\\\\nnxppa:{\\\\nmaterial(s):[ yggo-nibeqzrav-qbyz] }', 'skylight:{\\\\ncouca:{\\\\nrotation: 39-7-2 ] -- [435-838-27}', 'directional_light:{\\\\nzqkaz:{\\\\nrotation: 62-6-84 ] -- [839-8-449--\\\\nintensity: 748--\\\\nintensity: 357}--\\\\nintensity: 808}} }--\\\\nterrain:{\\\\nczol:{\\\\nposition:{ 883775 }--\\\\n position:{ 426160 }--\\\\n amount:[ 4380]-- [4756] ', 'object:{ \\\\ndcgf:{\\\\namount:[ 3536]-- [7611]   }--\\\\nterrain:{\\\\nnsfy:{\\\\nextent:[ 646-906-2] ', 'object:{ \\\\njyuuus:{\\\\namount:[ 7349]-- [4899] --\\\\namount:[ 6706]-- [5936]  }', 'object:{ \\\\nxzvwf:{\\\\nrotation:[87-037-73] --\\\\n extent:[ 4-22-8] --\\\\noverlap: true --\\\\nextent:[ 2-70-740] --\\\\nvisibility: false --\\\\namount:[ 6632]-- [1961] --/nclass(s):[ aiiwnt-lcom ]--\\\\nscale: [6] --/nposition:{ 9728273 }--\\\\namount:[ 2740]-- [1597] --\\\\n overlap: true --\\\\nvisibility: false --\\\\n class(s):[ zeotqshgersvcj-gcwlrvirrkgn-rltkd ]--/nmaterial(s):[ vuil] --\\\\nmaterial(s):[ wfajmou-yyjkcozfa-bcayj] --\\\\n overlap: false --/nextent:[ 03-901-9] --\\\\nrotation:[54-648-361 ] -- [99-776-549] --\\\\nposition:{ 7656 }--\\\\nposition:{ 9484190 }--\\\\namount:[ 1208]-- [0621] --\\\\n extent:[ 22-429-188]   }--\\\\nterrain:{\\\\nxgdctul:{\\\\nvisibility: false ', 'object:{ \\\\ncotq:{\\\\nscale: [6 -- 7 ]  }', 'directional_light:{\\\\nxelwfm:{\\\\nintensity: 099 }--\\\\nterrain:{\\\\nifxjcr:{\\\\namount:[ 2307]-- [0934] --\\\\nrotation:[45-5-79] --\\\\nclass(s):[ czzojwjoar-jmqnecugv-kool ]', 'skylight:{\\\\nqvasusiy:{\\\\nrotation: 822-571-720 ] -- [2-36-1--\\\\nrotation: 124-4-13 ] -- [96-21-74} }--\\\\nterrain:{\\\\njzer:{\\\\nextent:[ 95-65-8] ', 'terrain:{\\\\nvgya:{\\\\nclass(s):[ rpjbjhap-gpme ] }--\\\\ndirectional_light:{\\\\ntkhc:{\\\\nintensity: 864', 'directional_light:{\\\\nsztv:{\\\\nintensity: 237--\\\\nrotation: 137-545-09 ] -- [3-729-947}}', 'skylight:{\\\\nbaddh:{\\\\nintensity: 986--\\\\nintensity: 805} }--\\\\nskylight:{\\\\nqnqs:{\\\\nintensity: 719', 'directional_light:{\\\\nemzy:{\\\\nrotation: 1-6-87 ] -- [23-407-28--\\\\nintensity: 775}--\\\\nintensity: 018}}', 'directional_light:{\\\\ndjck:{\\\\nrotation: 5-1-49}', 'skylight:{\\\\ndtuce:{\\\\nintensity: 658}', 'directional_light:{\\\\nchko:{\\\\nrotation: 010-9-0--\\\\nrotation: 2-9-848--\\\\nintensity: 085}--\\\\nrotation: 16-09-35 ] -- [947-5-36}--\\\\nintensity: 945}--\\\\nintensity: 795}}--\\\\nintensity: 398}--\\\\nintensity: 538}}', 'object:{ \\\\njdztz:{\\\\nrotation:[645-673-10 ] -- [008-533-940] --/nscale: [3 -- 6 ] --\\\\nmaterial(s):[ fuvsufd-vyjho] --\\\\nrotation:[413-08-37]   }--\\\\nterrain:{\\\\nifit:{\\\\namount:[ 7919]-- [8276] ', 'terrain:{\\\\nihry:{\\\\nextent:[ 46-211-5]  }--\\\\nterrain:{\\\\ndxaeo:{\\\\nscale: [3] --\\\\nclass(s):[ cbly-nkpq-tybawtzng-jmfo-ygyn ]', 'directional_light:{\\\\ncknq:{\\\\nrotation: 1-7-9--\\\\nrotation: 94-88-03 ] -- [000-837-3--\\\\nrotation: 42-55-18 ] -- [2-27-636}--\\\\nintensity: 686}}--\\\\nrotation: 22-84-18}--\\\\nintensity: 895} }--\\\\nskylight:{\\\\neevc:{\\\\nrotation: 994-00-39', 'directional_light:{\\\\nwahu:{\\\\nrotation: 9-0-795 }--\\\\nterrain:{\\\\nvves:{\\\\noverlap: false ', 'terrain:{\\\\nnhktl:{\\\\nmaterial(s):[ bnukvqs]  }--\\\\nobject:{ \\\\ntfjzr:{\\\\nextent:[ 2-27-084]  ', 'skylight:{\\\\njhjvc:{\\\\nrotation: 1-524-2 ] -- [04-205-435}', 'object:{ \\\\nskfctazq:{\\\\nclass(s):[ guki-ejwdomnx ] }', 'directional_light:{\\\\nplcnwps:{\\\\nintensity: 623--\\\\nrotation: 4-65-9}}', 'object:{ \\\\nnmfxxtu:{\\\\nclass(s):[ wjroezibn-lcuopt ]  }--\\\\nterrain:{\\\\nkxiz:{\\\\noverlap: false ', 'object:{ \\\\ncmqu:{\\\\nmaterial(s):[ oypmcrhlv-zoua] --\\\\nvisibility: true   }--\\\\nobject:{ \\\\nrgey:{\\\\nscale: [4 -- 5 ]  ', 'skylight:{\\\\nzrwhuyi:{\\\\nrotation: 99-69-3 ] -- [675-1-3 }--\\\\nterrain:{\\\\ningb:{\\\\nrotation:[21-47-0 ] -- [9-877-579] --\\\\nscale: [7] --/nposition:{ 842941 }--\\\\nmaterial(s):[ woqiswzrourz-uigo] --/namount:[ 8617]-- [7417] --\\\\nextent:[ 191-376-0] ', 'terrain:{\\\\nhnjvr:{\\\\nextent:[ 621-522-314] --\\\\n rotation:[4-0-8 ] -- [015-465-562] }', 'terrain:{\\\\nkrog:{\\\\noverlap: false }', 'object:{ \\\\ncaky:{\\\\nextent:[ 48-762-40]   }--\\\\nterrain:{\\\\nbjjz:{\\\\nposition:{ \\\\non: ykcqm--\\\\non: kgcil--\\\\nabove: bdcel--\\\\non: jzhr--\\\\nin: fefc--\\\\nin: gqdn }--\\\\nextent:[ 2-695-20] ', 'skylight:{\\\\nwioer:{\\\\nintensity: 863}', 'skylight:{\\\\npeix:{\\\\nrotation: 633-7-69 ] -- [13-080-091--\\\\nrotation: 854-00-4} }--\\\\nobject:{ \\\\nkxqd:{\\\\nrotation:[0-0-5 ] -- [5-05-01]  ', 'terrain:{\\\\njbenr:{\\\\noverlap: true }', 'skylight:{\\\\nxjunz:{\\\\nrotation: 8-6-96 ] -- [133-96-6 }--\\\\nobject:{ \\\\nthxm:{\\\\nmaterial(s):[ nexkus-imil] --\\\\nrotation:[3-56-425]  ', 'directional_light:{\\\\nnrohvia:{\\\\nintensity: 284 }--\\\\nterrain:{\\\\njmgb:{\\\\nrotation:[584-341-2] ', 'object:{ \\\\nveho:{\\\\nrotation:[519-99-765]   }--\\\\ndirectional_light:{\\\\nzave:{\\\\nintensity: 368', 'terrain:{\\\\nxxube:{\\\\nvisibility: true  }--\\\\nskylight:{\\\\ndgjf:{\\\\nintensity: 133--\\\\nintensity: 994}', 'object:{ \\\\nnndf:{\\\\nrotation:[281-75-355]  }', 'directional_light:{\\\\nmyum:{\\\\nrotation: 45-672-693 }--\\\\nskylight:{\\\\ntezg:{\\\\nrotation: 37-34-751', 'terrain:{\\\\nmlnnbw:{\\\\nrotation:[71-775-405] }', 'directional_light:{\\\\nynzpoj:{\\\\nrotation: 35-985-55 ] -- [5-8-7 }--\\\\ndirectional_light:{\\\\nwtiq:{\\\\nintensity: 571', 'terrain:{\\\\nbaglpe:{\\\\nscale: [8] --\\\\nrotation:[96-4-1] --/nclass(s):[ tzrr ]--\\\\n scale: [0] --\\\\nscale: [3 -- 9 ] --\\\\n visibility: true --\\\\noverlap: false --\\\\noverlap: true --\\\\nextent:[ 233-150-3] --\\\\n material(s):[ wlyrivc] --/nrotation:[29-36-061] --/nposition:{ 031106 }--\\\\nvisibility: false --\\\\nrotation:[198-534-9 ] -- [84-115-874]  }--\\\\nobject:{ \\\\nrmoy:{\\\\namount:[ 8718]-- [2088] --\\\\namount:[ 5565]-- [6426] --\\\\namount:[ 1518]-- [3041] --\\\\nclass(s):[ nrpsl ]--\\\\nextent:[ 02-251-3] --/nscale: [8 -- 1 ] --\\\\nscale: [8 -- 2 ] --/nvisibility: false --\\\\nextent:[ 87-7-592] --\\\\nposition:{ \\\\nin: iwqa--\\\\nabove: rmnbvqlerc--\\\\non: ntxgzjzzc--\\\\nabove: nwbldtb--\\\\nin: hvqeymup--\\\\non: ottknb--\\\\nin: xdnivpy--\\\\nin: uoingu--\\\\non: vdiw--\\\\non: cgvnp--\\\\nabove: okkk--\\\\nin: eost--\\\\nabove: jvtl--\\\\nin: njan--\\\\nabove: cbpic--\\\\nabove: kfzcw--\\\\non: cnga--\\\\nabove: vxto--\\\\nabove: zhldcg--\\\\non: cmcj }--\\\\nmaterial(s):[ jtdxwt-vxkt] --\\\\nscale: [5 -- 4 ] --/nrotation:[5-888-666] --\\\\nscale: [6 -- 1 ] --\\\\nmaterial(s):[ wbryybokydq-jawb] --\\\\nextent:[ 44-6-7] --\\\\nvisibility: true --\\\\nclass(s):[ ihfaalnfaqg-ktfq-flrwq-yvkl ]--\\\\namount:[ 0387]-- [0493] --\\\\n material(s):[ yclw] --\\\\nvisibility: true --\\\\noverlap: false --\\\\nextent:[ 6-57-73] --\\\\nscale: [0 -- 8 ] --/nscale: [0] --\\\\nmaterial(s):[ mlrjvj-wrixq] --\\\\nextent:[ 59-9-2] --\\\\nmaterial(s):[ krqssyigsthnj-mxan]  ', 'object:{ \\\\nntgjuo:{\\\\namount:[ 3746]-- [3687]  }', 'directional_light:{\\\\normg:{\\\\nrotation: 37-918-085 ] -- [257-4-95--\\\\nintensity: 762}--\\\\nintensity: 179} }--\\\\ndirectional_light:{\\\\nfkhm:{\\\\nintensity: 411', 'terrain:{\\\\nwjazhybh:{\\\\nclass(s):[ sqsxbxnvec-ljbtkmwspn-iudus ]--\\\\nposition:{ \\\\nin: dbfo--\\\\nin: rzkp--\\\\nin: zemjsv--\\\\nabove: cvlorg--\\\\non: zdxc--\\\\non: cysj--\\\\nin: jlfumb--\\\\nabove: rhhe--\\\\nin: jmhn--\\\\non: xucz--\\\\non: qxvf--\\\\nabove: btvl--\\\\non: utlessve--\\\\nin: twypfdtwg--\\\\non: oibb }}', 'directional_light:{\\\\nsintpgue:{\\\\nrotation: 7-8-5 }--\\\\ndirectional_light:{\\\\nzwxweh:{\\\\nintensity: 187--\\\\nintensity: 721--\\\\nintensity: 445--\\\\nintensity: 244}}--\\\\nintensity: 851--\\\\nintensity: 847}}--\\\\nintensity: 778}}', 'object:{ \\\\nenfhmpj:{\\\\nmaterial(s):[ ngkbvuemcle-pfveqgzo] --\\\\namount:[ 8098]-- [4300] --\\\\nclass(s):[ wbfhbjoml-spzr-sttyq-ivit ]--\\\\namount:[ 6365]-- [0971] --/nrotation:[789-8-73 ] -- [663-036-0] --\\\\nrotation:[426-449-65] --\\\\nposition:{ \\\\nabove: ukyu--\\\\nin: broc--\\\\non: pykb--\\\\non: mifx--\\\\nabove: iwxz--\\\\nin: oifw--\\\\nabove: yfgdb--\\\\nabove: icvh--\\\\non: ckbjmq--\\\\nin: kzod--\\\\non: ctfm--\\\\nabove: hiopc--\\\\nin: dfnqt--\\\\nin: issne--\\\\nin: ltxz--\\\\non: kbnzg--\\\\nabove: wkyy--\\\\nin: rxuve--\\\\nabove: pxact--\\\\non: wsvfod--\\\\non: ptgmu--\\\\non: ayml--\\\\non: ujvr--\\\\nabove: tbnku--\\\\nabove: ztku--\\\\nabove: lndn--\\\\nin: vrgi--\\\\non: hbotz--\\\\nin: iwdlt--\\\\non: ojzx--\\\\nabove: ulke--\\\\nin: ehuo--\\\\nin: exilwd--\\\\nabove: mouojf--\\\\nin: slxbxwmf--\\\\nin: byfr--\\\\nabove: zibc--\\\\non: fcfa--\\\\non: hdgo--\\\\nabove: peuwz--\\\\non: wsgn--\\\\non: gekfzc--\\\\nabove: snmh--\\\\nin: pzpa--\\\\nabove: jbnik--\\\\nin: rexfzl--\\\\nabove: ayvb--\\\\non: gcwoj--\\\\nin: skxa--\\\\non: mnbn--\\\\non: kgci--\\\\nin: mepak--\\\\non: nseo--\\\\nabove: zaysopo--\\\\non: pldv--\\\\nin: tzkt--\\\\nabove: mqsrf--\\\\non: bmox--\\\\nabove: fhvsa--\\\\non: jzdsi--\\\\nin: tznq--\\\\nabove: dmclj--\\\\nin: bfqm--\\\\nin: qvvxg--\\\\nabove: gall--\\\\nabove: lyxw } }', 'object:{ \\\\nwvrxtvhgt:{\\\\noverlap: true  }', 'terrain:{\\\\namvy:{\\\\nmaterial(s):[ dusux-fmtz] --/nposition:{ \\\\nabove: wynue }--\\\\nrotation:[561-2-3] }', 'terrain:{\\\\ngusiis:{\\\\nscale: [2 -- 3 ] --\\\\nmaterial(s):[ lgyj-ynvln] --\\\\nextent:[ 063-71-01] --\\\\noverlap: true }', 'directional_light:{\\\\nsyav:{\\\\nintensity: 240 }--\\\\nskylight:{\\\\niios:{\\\\nintensity: 621', 'terrain:{\\\\nlczagyju:{\\\\namount:[ 7169]-- [7910] --\\\\nscale: [6 -- 6 ]  }--\\\\nobject:{ \\\\nfjvv:{\\\\nvisibility: true  ']\n",
      "['hey how are you', 'good i am fine', 'have a nice day']\n"
     ]
    }
   ],
   "source": [
    "text3 = ['hey how are you','good i am fine','have a nice day']\n",
    "df = pd.read_csv('../sentences.csv')\n",
    "text = df['input'].tolist()\n",
    "text2 = df['target'].tolist()\n",
    "input_seq = df['input'].tolist()\n",
    "target_seq = df['target'].tolist()\n",
    "text = text + text2\n",
    "print(text)\n",
    "print(text3)\n",
    "\n",
    "# Join all the sentences together and extract the unique characters from the combined sentences\n",
    "#chars = set(''.join(text))\n",
    "chars = set(''.join(text))\n",
    "\n",
    "# Creating a dictionary that maps integers to the characters\n",
    "int2char = dict(enumerate(chars))\n",
    "#int2char = dict(enumerate(chars))\n",
    "\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'u': 0, 'g': 1, '6': 2, 'm': 3, ':': 4, 'a': 5, ']': 6, 'r': 7, '3': 8, 'c': 9, 'q': 10, 'p': 11, '{': 12, '4': 13, '5': 14, 'x': 15, '9': 16, '/': 17, 'z': 18, 'e': 19, 't': 20, 'b': 21, '-': 22, 'y': 23, '_': 24, 'd': 25, ' ': 26, '8': 27, '2': 28, 'h': 29, '7': 30, '0': 31, 'w': 32, 'l': 33, 'n': 34, '}': 35, 's': 36, '[': 37, 'j': 38, 'i': 39, '(': 40, '\\\\': 41, ')': 42, '1': 43, 'f': 44, 'k': 45, 'o': 46, 'v': 47}\n"
     ]
    }
   ],
   "source": [
    "print(char2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll be padding our input sentences to ensure that all the sentences are of the sample length. While RNNs are typically able to take in variably sized inputs, we will usually want to feed training data in batches to speed up the training process. In order to used batches to train on our data, we'll need to ensure that each sequence within the input data are of equal size.\n",
    "\n",
    "Therefore, in most cases, padding can be done by filling up sequences that are too short with **0** values and trimming sequences that are too long. In our case, we'll be finding the length of the longest sequence and padding the rest of the sentences with blank spaces to match that length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest string has 1344 characters\n"
     ]
    }
   ],
   "source": [
    "maxlen = len(max(text, key=len))\n",
    "print(\"The longest string has {} characters\".format(maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# Padding\n",
    "\n",
    "# A simple loop that loops through the list of sentences and adds a ' ' whitespace until the length of the sentence matches\n",
    "# the length of the longest sentence\n",
    "half = len(text) - len(input_seq)\n",
    "print(half)\n",
    "for i in range(half):\n",
    "    while len(input_seq[i])<maxlen:\n",
    "        input_seq[i] += ' '\n",
    "    while len(target_seq[i])<maxlen:\n",
    "        target_seq[i] += ' '\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we're going to predict the next character in the sequence at each time step, we'll have to divide each sentence into\n",
    "\n",
    "- Input data\n",
    "    - The last input character should be excluded as it does not need to be fed into the model\n",
    "- Target/Ground Truth Label\n",
    "    - One time-step ahead of the Input data as this will be the \"correct answer\" for the model at each time step corresponding to the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lists that will hold our input and target sequences\n",
    "# input_seq = []\n",
    "# target_seq = []\n",
    "\n",
    "# for i in range(len(text)):\n",
    "#     # Remove last character for input sequence\n",
    "#     input_seq.append(text[i][:-1])\n",
    "    \n",
    "#     # Remove firsts character for target sequence\n",
    "#     target_seq.append(text[i][1:])\n",
    "#     print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert our input and target sequences to sequences of integers instead of characters by mapping them using the dictionaries we created above. This will allow us to one-hot-encode our input sequence subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(half):\n",
    "        input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "        target_seq[i] = [char2int[character] for character in target_seq[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before encoding our input sequence into one-hot vectors, we'll define 3 key variables:\n",
    "\n",
    "- *dict_size*: The number of unique characters that we have in our text\n",
    "    - This will determine the one-hot vector size as each character will have an assigned index in that vector\n",
    "- *seq_len*: The length of the sequences that we're feeding into the model\n",
    "    - As we standardised the length of all our sentences to be equal to the longest sentences, this value will be the max length - 1 as we removed the last character input as well\n",
    "- *batch_size*: The number of sentences that we defined and are going to feed into the model as a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_size = len(char2int)\n",
    "seq_len = maxlen-1\n",
    "batch_size = half\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also defined a helper function that creates arrays of zeros for each character and replaces the corresponding character index with a **1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, half)\n",
    "target_seq = one_hot_encode(target_seq, dict_size, seq_len, half)\n",
    "\n",
    "#print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're done with all the data pre-processing, we can now move the data from numpy arrays to PyTorch's very own data structure - **Torch Tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've reached the fun part of this project! We'll be defining the model using the Torch library, and this is where you can add or remove layers, be it fully connected layers, convolutational layers, vanilla RNN layers, LSTM layers, and many more! In this post, we'll be using the basic nn.rnn to demonstrate a simple example of how RNNs can be used.\n",
    "\n",
    "Before we start building the model, let's use a build in feature in PyTorch to check the device we're running on (CPU or GPU). This implementation will not require GPU as the training is really simple. However, as you progress on to large datasets and models with millions of trainable parameters, using the GPU will be very important to speed up your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start building our own neural network model, we can define a class that inherits PyTorchâ€™s base class (nn.module) for all neural network modules. After doing so, we can start defining some variables and also the layers for our model under the constructor. For this model, weâ€™ll only be using 1 layer of RNN followed by a fully connected layer. The fully connected layer will be in-charge of converting the RNN output to our desired output shape.\n",
    "\n",
    "Weâ€™ll also have to define the forward pass function under forward() as a class method. The order the forward function is sequentially executed, therefore weâ€™ll have to pass the inputs and the zero-initialized hidden state through the RNN layer first, before passing the RNN outputs to the fully-connected layer. Note that we are using the layers that we defined in the constructor.\n",
    "\n",
    "The last method that we have to define is the method that we called earlier to initialize the hidden state - init_hidden(). This basically creates a tensor of zeros in the shape of our hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True, nonlinearity='relu')   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "\n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        #out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        #print(\"out 1\",out.shape)\n",
    "        #out = self.fc(out)\n",
    "        #print(\"out 2\", out.shape)\n",
    "        #print(\"hidden\", hidden.shape)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model above, we'll have to instantiate the model with the relevant parameters and define our hyperparamters as well. The hyperparameters we're defining below are:\n",
    "\n",
    "- *n_epochs*: Number of Epochs --> This refers to the number of times our model will go through the entire training dataset\n",
    "- *lr*: Learning Rate --> This affects the rate at which our model updates the weights in the cells each time backpropogation is done\n",
    "    - A smaller learning rate means that the model changes the values of the weight with a smaller magnitude\n",
    "    - A larger learning rate means that the weights are updated to a larger extent for each time step\n",
    "\n",
    "Similar to other neural networks, we have to define the optimizer and loss function as well. Weâ€™ll be using CrossEntropyLoss as the final output is basically a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "\n",
    "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=30, n_layers=2)\n",
    "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define hyperparameters\n",
    "n_epochs = 300\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin our training! As we only have a few sentences, this training process is very fast. However, as we progress, larger datasets and deeper models mean that the input data is much larger and the number of parameters within the model that we have to compute is much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/300............. Loss: 0.6909\n",
      "Epoch: 20/300............. Loss: 0.4974\n",
      "Epoch: 30/300............. Loss: 0.4257\n",
      "Epoch: 40/300............. Loss: 0.4090\n",
      "Epoch: 50/300............. Loss: 0.4026\n",
      "Epoch: 60/300............. Loss: 0.3987\n",
      "Epoch: 70/300............. Loss: 0.3945\n",
      "Epoch: 80/300............. Loss: 0.3970\n",
      "Epoch: 90/300............. Loss: 0.3909\n",
      "Epoch: 100/300............. Loss: 0.3860\n",
      "Epoch: 110/300............. Loss: 0.3817\n",
      "Epoch: 120/300............. Loss: 0.3770\n",
      "Epoch: 130/300............. Loss: 0.3867\n",
      "Epoch: 140/300............. Loss: 0.3817\n",
      "Epoch: 150/300............. Loss: 0.3782\n",
      "Epoch: 160/300............. Loss: 0.3747\n",
      "Epoch: 170/300............. Loss: 0.3715\n",
      "Epoch: 180/300............. Loss: 0.3687\n",
      "Epoch: 190/300............. Loss: 0.3659\n",
      "Epoch: 200/300............. Loss: 0.3733\n",
      "Epoch: 210/300............. Loss: 0.3625\n",
      "Epoch: 220/300............. Loss: 0.4946\n",
      "Epoch: 230/300............. Loss: 0.4370\n",
      "Epoch: 240/300............. Loss: 0.4028\n",
      "Epoch: 250/300............. Loss: 0.3935\n",
      "Epoch: 260/300............. Loss: 0.3851\n",
      "Epoch: 270/300............. Loss: 0.4166\n",
      "Epoch: 280/300............. Loss: 0.3963\n",
      "Epoch: 290/300............. Loss: 0.5183\n",
      "Epoch: 300/300............. Loss: 0.3843\n"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "input_seq = input_seq.to(device)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    #input_seq = input_seq.to(device)\n",
    "    output, hidden = model(input_seq)\n",
    "    output = output.to(device)\n",
    "    target_seq = target_seq.to(device)\n",
    "    # print(\"output before\",output.shape)\n",
    "    # print(\"targ before\", target_seq.shape)\n",
    "    # print(\"targ\", target_seq.view(-1).long().shape)\n",
    "    targ = target_seq.reshape(target_seq.size(0) * target_seq.size(1), -1)\n",
    "    # print(\"targ reshape\", targ.shape)\n",
    "    loss = criterion(output, targ)\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# define the RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        x = x.long()\n",
    "        embedded = self.embedding(x)\n",
    "        out, hidden = self.rnn(embedded, hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "# define the Word2Vec model\n",
    "class W2VModel():\n",
    "    def __init__(self, sentences):\n",
    "        self.model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        vecs = []\n",
    "        sentence2 = str(sentence)\n",
    "        sentences3 = sentence2.split()\n",
    "        for word in sentences3:\n",
    "            if word in self.model.wv.key_to_index:\n",
    "                vecs.append(self.model[word])\n",
    "        return torch.Tensor(vecs)\n",
    "\n",
    "# define the training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, w2v_model, optimizer, criterion, train_loader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            print(inputs)\n",
    "            encoded_inputs = w2v_model.encode(inputs)\n",
    "            encoded_targets = w2v_model.encode(targets)\n",
    "            print(\"in\",encoded_inputs)\n",
    "            print(\"out\",encoded_targets)\n",
    "            \n",
    "            outputs, _ = model(encoded_inputs.unsqueeze(0))\n",
    "            \n",
    "            loss = criterion(outputs, encoded_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i+1) % 10 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs and targs ['this is the input.', 'this is another input.', 'this is the target output.', 'this is another target output.']\n",
      "w2v Word2Vec(vocab=14, vector_size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sample data\n",
    "sentences = ['this is a sample sentence.', 'this is another sample sentence.']\n",
    "inputs = ['this is the input.', 'this is another input.']\n",
    "targets = ['this is the target output.', 'this is another target output.']\n",
    "allStuff = inputs + targets\n",
    "print(\"inputs and targs\", allStuff)\n",
    "# create the Word2Vec model\n",
    "w2v_model = W2VModel(allStuff)\n",
    "print(\"w2v\", w2v_model.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "[('this is the input.', 'this is the target output.'), ('this is another input.', 'this is another target output.')]\n"
     ]
    }
   ],
   "source": [
    "input_size = len(w2v_model.model.wv)\n",
    "print(input_size)\n",
    "hidden_size = 100\n",
    "output_size = 100\n",
    "num_layers = 1\n",
    "model = RNNModel(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "# define the optimizer and loss function\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# define the data loader\n",
    "train_data = [(inputs[i], targets[i]) for i in range(len(inputs))]\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "print(train_loader.dataset)\n",
    "# train the model\n",
    "num_epochs = 10\n",
    "train(model, w2v_model, optimizer, criterion, train_loader, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model is able to come up with the sentence â€˜good i am fine â€˜ if we feed it with the words â€˜goodâ€™, achieving what we intended for it to do!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "17ca0e2b29f2bcf16fed3be84e5de03177236cd844a3cd15600f8e36f2976c8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
